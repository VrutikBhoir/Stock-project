#!/usr/bin/env python
"""
Test and validate the ML-driven Narrative Engine.

This script:
1. Trains the ML model (if not already trained)
2. Tests narrative generation for multiple stocks
3. Validates that outputs are ML-driven, not mocked
4. Checks confidence varies across different stocks

Usage:
    python backend/test_narrative_engine.py
"""

import os
import sys
import json
import logging
from pathlib import Path

# Setup path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
os.chdir(os.path.dirname(__file__))

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

# Test stocks with different profiles
TEST_CASES = [
    {"symbol": "AAPL", "investor_type": "Balanced", "horizon": "medium_term", "description": "Large-cap tech"},
    {"symbol": "NVDA", "investor_type": "Aggressive", "horizon": "short_term", "description": "Growth/AI sector"},
    {"symbol": "JNJ", "investor_type": "Conservative", "horizon": "long_term", "description": "Defensive/Healthcare"},
    {"symbol": "TSLA", "investor_type": "Aggressive", "horizon": "short_term", "description": "High volatility"},
    {"symbol": "PG", "investor_type": "Conservative", "horizon": "long_term", "description": "Blue-chip stable"},
]


def test_models_trained():
    """Verify that trained models exist."""
    logger.info("\nüîç Checking if models are trained...")
    
    model_path = Path("app/services/ml/models/narrative_engine_final.pkl")
    
    if not model_path.exists():
        logger.warning(f"‚ö†Ô∏è  Model not found at {model_path}")
        logger.info("   Training model now...")
        
        import subprocess
        result = subprocess.run(
            [sys.executable, "train_narrative_model.py"],
            cwd=".",
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            logger.error(f"‚ùå Training failed:\n{result.stderr}")
            return False
        
        logger.info(result.stdout)
    
    if model_path.exists():
        size_mb = model_path.stat().st_size / (1024 * 1024)
        logger.info(f"‚úÖ Model exists: {model_path} ({size_mb:.2f} MB)")
        return True
    else:
        logger.error("‚ùå Model file not created")
        return False


def test_narrative_generation():
    """Test narrative generation for multiple stocks."""
    logger.info("\nüìä Testing narrative generation...")
    
    try:
        from app.services.ml.narrative_engine import get_narrative_engine
        from app.services.ml.price_predictor import get_prediction_with_analysis
    except ImportError as e:
        logger.error(f"‚ùå Import failed: {str(e)}")
        return False
    
    results = []
    
    for test_case in TEST_CASES:
        symbol = test_case["symbol"]
        investor_type = test_case["investor_type"]
        horizon = test_case["horizon"]
        description = test_case["description"]
        
        logger.info(f"\n  Testing {symbol} ({description})...")
        
        try:
            # Get prediction data
            analysis_result = get_prediction_with_analysis(
                symbol=symbol,
                investment_horizon=horizon
            )
            
            if not analysis_result.get("prediction") or not analysis_result.get("investment_analysis"):
                logger.warning(f"    ‚ö†Ô∏è Could not fetch data for {symbol}")
                continue
            
            # Generate narrative
            engine = get_narrative_engine()
            narrative = engine.generate_from_prediction(
                symbol=symbol,
                prediction_data=analysis_result["prediction"],
                analysis_data=analysis_result["investment_analysis"],
                investor_type=investor_type,
                time_horizon=horizon
            )
            
            # Extract key values
            sentiment = narrative.get("narrative", {}).get("sentiment")
            confidence = narrative.get("narrative", {}).get("confidence")
            conviction = narrative.get("narrative", {}).get("conviction")
            signal_strength = narrative.get("narrative", {}).get("signal_strength")
            model_used = narrative.get("model_used", "")
            
            # Validation checks
            checks = {
                "sentiment_valid": sentiment in ["Bullish", "Neutral", "Bearish"],
                "confidence_valid": isinstance(confidence, (int, float)) and 0 <= confidence <= 100,
                "conviction_valid": conviction in ["High", "Medium", "Low"],
                "signal_strength_valid": signal_strength in ["Strong", "Moderate", "Weak"],
                "model_correct": "narrative_engine_final.pkl" in model_used,
            }
            
            all_valid = all(checks.values())
            status = "‚úÖ" if all_valid else "‚ùå"
            
            logger.info(f"    {status} {symbol}: {sentiment} ({confidence:.1f}% conf, {conviction} conviction)")
            
            # Log validation details
            if not all_valid:
                for check_name, result in checks.items():
                    if not result:
                        logger.warning(f"       ‚ö†Ô∏è {check_name} failed")
            
            results.append({
                "symbol": symbol,
                "sentiment": sentiment,
                "confidence": confidence,
                "conviction": conviction,
                "signal_strength": signal_strength,
                "all_valid": all_valid
            })
        
        except Exception as e:
            logger.error(f"    ‚ùå Error for {symbol}: {str(e)}")
            results.append({"symbol": symbol, "error": str(e), "all_valid": False})
    
    return results


def validate_ml_is_active(results):
    """
    Validate that ML models are actually being used.
    
    Checks:
    1. Confidence varies across stocks (not all 49%, 50%, etc.)
    2. Same stock can have different confidence with different investors
    3. No hardcoded values
    """
    logger.info("\nüß™ Validating ML is active (not mocked)...")
    
    confident_values = [r.get("confidence") for r in results if r.get("confidence") is not None]
    
    if not confident_values:
        logger.error("‚ùå No valid confidence scores to validate")
        return False
    
    # Check 1: Confidence varies
    min_conf = min(confident_values)
    max_conf = max(confident_values)
    conf_range = max_conf - min_conf
    
    if conf_range < 5:
        logger.warning(f"‚ö†Ô∏è Confidence range very small ({min_conf:.1f}% - {max_conf:.1f}%)")
        logger.warning("   This suggests ML may not be differentiating properly")
        return False
    else:
        logger.info(f"‚úÖ Confidence varies meaningfully: {min_conf:.1f}% to {max_conf:.1f}% (range: {conf_range:.1f}%)")
    
    # Check 2: No hardcoded mock values
    mock_values = [49, 50, 51, 49.5, 50.5]
    hardcoded_count = sum(1 for c in confident_values if abs(c - 50) < 1)
    
    if hardcoded_count > len(confident_values) * 0.5:
        logger.error("‚ùå Too many confidence scores near 50% (looks like defaults)")
        return False
    else:
        logger.info(f"‚úÖ Confidence values look natural (no excessive defaults)")
    
    # Check 3: Sentiment varies
    sentiments = [r.get("sentiment") for r in results if r.get("sentiment")]
    unique_sentiments = set(sentiments)
    
    logger.info(f"‚úÖ Found {len(unique_sentiments)} different sentiments: {unique_sentiments}")
    
    # Check 4: All valid
    all_valid = all(r.get("all_valid") for r in results if "all_valid" in r)
    if all_valid:
        logger.info("‚úÖ All narrative validations passed")
    else:
        logger.warning("‚ö†Ô∏è Some validations failed")
    
    return True


def main():
    """Run all tests."""
    logger.info("=" * 70)
    logger.info("üöÄ ML-Driven Narrative Engine Test Suite")
    logger.info("=" * 70)
    
    # Test 1: Models trained
    if not test_models_trained():
        logger.error("‚ùå Model training failed")
        sys.exit(1)
    
    # Test 2: Narrative generation
    results = test_narrative_generation()
    
    if not results:
        logger.error("‚ùå No test results generated")
        sys.exit(1)
    
    valid_results = [r for r in results if r.get("all_valid")]
    logger.info(f"\n‚úÖ Generated {len(valid_results)}/{len(results)} valid narratives")
    
    # Test 3: Validate ML is active
    if validate_ml_is_active(results):
        logger.info("\n‚úÖ ML models are ACTIVE and generating varied outputs")
    else:
        logger.warning("\n‚ö†Ô∏è ML validation checks failed‚Äîcheck model training")
    
    # Summary
    logger.info("\n" + "=" * 70)
    logger.info("üìä Test Summary")
    logger.info("=" * 70)
    
    for result in results:
        symbol = result.get("symbol", "UNKNOWN")
        if "error" in result:
            logger.info(f"  ‚ùå {symbol}: {result['error']}")
        else:
            conf = result.get("confidence", "?")
            sent = result.get("sentiment", "?")
            conv = result.get("conviction", "?")
            logger.info(f"  ‚úÖ {symbol}: {sent} ({conf:.1f}% conf, {conv})")
    
    logger.info("\nüéâ Test complete! Narrative engine is ready for production.")
    logger.info("   Remove the narrative_engine_final.pkl file to trigger model retraining.")


if __name__ == "__main__":
    main()
